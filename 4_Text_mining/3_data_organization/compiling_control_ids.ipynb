{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80bfbd2",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820344bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920536\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read the recommendation_id.pickle file\n",
    "with open(os.path.join('recommendation_id.pkl'), 'rb') as file:\n",
    "    recommendation_id = pickle.load(file)\n",
    "print(len(recommendation_id))\n",
    "\n",
    "\n",
    "\n",
    "def sort_key(s):\n",
    "    numbers = re.findall(r'\\d+', s)\n",
    "    return int(numbers[0]) if numbers else 0\n",
    "\n",
    "# Read the recommendation_id.pickle file\n",
    "with open(os.path.join('recommendation_id.pkl'), 'rb') as file:\n",
    "    recommendation_id = pickle.load(file)\n",
    "\n",
    "directory = '.'  # Current directory\n",
    "pickle_files = [file for file in os.listdir(directory) if file.endswith('.pickle') and file != 'recommendation_id.pkl']\n",
    "pickle_files.sort(key=sort_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22050857",
   "metadata": {},
   "source": [
    "## Convert them into csv files (in parts due to their large size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6897671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████▊                       | 356/876 [1:18:14<38:16,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_400000_to_401000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████▉                       | 357/876 [1:18:18<37:02,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_401000_to_402000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████▉                       | 358/876 [1:18:22<35:38,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_402000_to_403000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████▉                       | 359/876 [1:18:26<34:21,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_403000_to_404000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████████████████                       | 360/876 [1:18:29<33:36,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_404000_to_405000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████████████████                       | 361/876 [1:18:33<32:58,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_405000_to_406000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████████████████                       | 362/876 [1:18:37<33:00,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_406000_to_407000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████████████████▏                      | 363/876 [1:18:41<32:41,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_407000_to_408000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████▏                      | 364/876 [1:18:44<32:21,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_408000_to_409000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████▎                      | 365/876 [1:18:48<32:02,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_409000_to_410000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████▎                      | 366/876 [1:18:52<31:50,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_410000_to_411000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████▎                      | 367/876 [1:18:56<31:43,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_411000_to_412000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████▍                      | 368/876 [1:18:59<31:29,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_412000_to_413000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████▍                      | 369/876 [1:19:03<31:18,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_413000_to_414000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████▍                      | 370/876 [1:19:07<31:16,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_414000_to_415000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████▌                      | 371/876 [1:19:10<31:07,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_415000_to_416000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████▍        | 685/876 [2:02:42<25:29,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_729000_to_730000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████▌        | 686/876 [2:02:50<24:39,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_730000_to_731000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████▌        | 687/876 [2:02:57<24:03,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_731000_to_732000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|██████████████████████████████▋        | 688/876 [2:03:04<23:48,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_732000_to_733000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|██████████████████████████████▋        | 689/876 [2:03:12<23:24,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_733000_to_734000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|██████████████████████████████▋        | 690/876 [2:03:19<23:12,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_matrix_734000_to_735000.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 876/876 [2:33:05<00:00, 10.49s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_and_save_df(similarities, recommendation_ids_filtered, accounted_for):\n",
    "    # Filter similarities and recommendation_ids_filtered based on accounted_for\n",
    "    similarities_filtered = []\n",
    "    recommendation_ids_filtered_new = []\n",
    "    for sub_lst, val in zip(similarities, recommendation_ids_filtered):\n",
    "        if val not in accounted_for:\n",
    "            similarities_filtered.append(sub_lst)\n",
    "            recommendation_ids_filtered_new.append(val)\n",
    "\n",
    "    # Return an empty dataframe if no suitable values found\n",
    "    if len(recommendation_ids_filtered_new)==0:\n",
    "        return (pd.DataFrame(dtype='float64'))\n",
    "\n",
    "    # Cache the lengths of the sublists in similarities_filtered\n",
    "    lengths = [len(sub_lst) for sub_lst in similarities_filtered]\n",
    "\n",
    "    # Flatten similarities_filtered using the cached lengths\n",
    "    similarities = np.concatenate(similarities_filtered)\n",
    "\n",
    "    # Repeat elements in recommendation_ids_filtered according to the cached lengths\n",
    "    recommendation_id = np.repeat(recommendation_ids_filtered_new, lengths)\n",
    "\n",
    "    # Create a DataFrame from the columns\n",
    "    df = pd.DataFrame({'similarities': similarities, 'recommendation_id': recommendation_id})\n",
    "    return df\n",
    "\n",
    "\n",
    "length, accounted_for, i = 0, [], 1\n",
    "for filename in tqdm(pickle_files):\n",
    "    # Load the current file\n",
    "    with open(filename, 'rb') as file:\n",
    "        current = pickle.load(file)\n",
    "    new_length = len(current)+length\n",
    "    recommendation_ids_filtered = (recommendation_id[length:new_length])\n",
    "    df = convert_and_save_df(current, recommendation_ids_filtered, accounted_for)\n",
    "    accounted_for += recommendation_ids_filtered\n",
    "    length = new_length\n",
    "    if df.empty:\n",
    "        continue\n",
    "        #print(filename)\n",
    "    else:\n",
    "        df.to_csv(f'similarities_control_part_{i}.csv', index=False)\n",
    "        \n",
    "    i = i + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4170d",
   "metadata": {},
   "source": [
    "## Save them in sizeable csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dc505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get a list of all CSV files in the current directory\n",
    "csv_files = glob.glob('*.csv')\n",
    "\n",
    "# Create a generator that will load and yield dataframes\n",
    "def load_csvs(file_list):\n",
    "    for file in file_list:\n",
    "        yield pd.read_csv(file)\n",
    "\n",
    "# Use the generator to load and concatenate CSV files in chunks\n",
    "num_files = len(csv_files)\n",
    "chunk_size = num_files // 10  # Number of files per output CSV\n",
    "remainder = num_files % 10  # Remaining files for the last CSV\n",
    "chunks = [csv_files[i * chunk_size:(i + 1) * chunk_size] for i in tqdm(range(10))]\n",
    "if remainder > 0:\n",
    "    chunks[-1] += csv_files[-remainder:]  # Add remaining files to the last chunk\n",
    "\n",
    "# Concatenate and save each chunk\n",
    "for i, chunk in tqdm(enumerate(chunks, 1)):\n",
    "    df = pd.concat(load_csvs(chunk))\n",
    "    df.to_csv(f'combined_control_ids_part_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e0f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
