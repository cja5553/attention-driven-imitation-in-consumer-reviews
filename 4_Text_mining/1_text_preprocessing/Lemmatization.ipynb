{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lemmatization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDR4zHCf84ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ebba097-8d06-4cd0-eb4c-b975fa39ec5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Dissertation\n",
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> omw-1.4\n",
            "Command 'omw-1.4' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> omw-1.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> u\n",
            "\n",
            "Nothing to update.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> omw-1.4\n",
            "Command 'omw-1.4' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# set up colab and read pickle file\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd '/content/drive/My Drive/Dissertation'\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import csv\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download() \n",
        "\n",
        "\n",
        "FPS_reviews = pd.read_csv('FPS_reviews_preprocessed.csv')\n",
        "\n",
        "# download nltk for lemmatization\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "#nltk.download()\n",
        "import math\n",
        "\n",
        "# perform lemmatization\n",
        "\n",
        "def lemmatize(selected_reviews):\n",
        "\n",
        "  def get_wordnet_pos(word):\n",
        "      \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "      tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "      tag_dict = {\"J\": wordnet.ADJ,\n",
        "                  \"N\": wordnet.NOUN,\n",
        "                  \"V\": wordnet.VERB,\n",
        "                 \"R\": wordnet.ADV}\n",
        "      return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer() \n",
        "  lemmatized_reviews=[]\n",
        "  for i in tqdm(range(0, len(selected_reviews))):\n",
        "    new_list_sentence = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(str(selected_reviews[i]))]\n",
        "    lemmatized_output = ' '.join([w for w in new_list_sentence])\n",
        "    lemmatized_reviews.append(lemmatized_output)\n",
        "  return(lemmatized_reviews)\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# execute the above function\n",
        "FPS_reviews[\"review\"]=lemmatize(list(FPS_reviews[\"review\"]))\n",
        "FPS_reviews.to_csv('FPS_reviews_lemmatized.csv')"
      ],
      "metadata": {
        "id": "WS1FpVMBT1_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f5a5de-2520-4b35-c4cc-8b67f47f2ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1056940/1056940 [1:20:24<00:00, 219.06it/s]\n"
          ]
        }
      ]
    }
  ]
}