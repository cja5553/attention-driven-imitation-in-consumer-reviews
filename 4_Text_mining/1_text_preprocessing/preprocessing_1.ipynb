{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbFZUgqV92bU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c4a964-5b7c-4b6c-b4c3-7f4351d4db4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Dissertation\n"
          ]
        }
      ],
      "source": [
        "# read pickle file and set up colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd '/content/drive/My Drive/Dissertation'\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import csv\n",
        "import pandas as pd \n",
        "\n",
        "FPS_reviews = pd.read_csv('FPS_reviews_list.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install and download packages for pre-processing\n",
        "\n",
        "!pip install emot\n",
        "!pip install emoji\n",
        "!pip install emot.emo_unicode\n",
        "!pip install gensim.parsing.preprocessing\n",
        "import re\n",
        "import pickle\n",
        "from emot import *\n",
        "import emoji\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from emot.emo_unicode import EMOJI_UNICODE\n",
        "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS\n"
      ],
      "metadata": {
        "id": "Dh2nbXaTQ0qw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8490fa-53f2-4604-9d9c-dda2d97a0c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 18 kB/s \n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.0.0.tar.gz (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 4.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.0.0-py3-none-any.whl size=193022 sha256=d116e10268f9d829e5f32fe81b5fc3c07869f7f400790e7cc84380fb8cb2a649\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/29/4d/3cfe7452ac7d8d83b1930f8a6205c3c9649b24e80f9029fc38\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement emot.emo_unicode (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for emot.emo_unicode\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement gensim.parsing.preprocessing (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for gensim.parsing.preprocessing\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deal with profanities profanities censored as ♥. \n",
        "def identify_profanities_1(selected_reviews):\n",
        "  with open(\"english-profanity.txt\") as file:\n",
        "    profanity_list=[]\n",
        "    for item in file:\n",
        "      profanity_list.append(item)\n",
        "  profanity_dictionary={}\n",
        "  for i in range(len(profanity_list)):\n",
        "    word=profanity_list[i]\n",
        "    censored=\"♥\"*int(len(word))\n",
        "# Create a dictionary which attempts to match the length of the profaned text with the actual text in the compiled list\n",
        "    profanity_dictionary[word]=censored\n",
        "  for i in tqdm(range(0, len(selected_reviews))):\n",
        "    review=str(selected_reviews[i])\n",
        "    for profanity in profanity_dictionary:\n",
        "      review=review.replace(profanity_dictionary[profanity], profanity)\n",
        "    selected_reviews[i]=review\n",
        "  return (selected_reviews)\n",
        "\n",
        "# deal with profanities censored as '*'\n",
        "def identify_profanities_2(selected_reviews):\n",
        "  with open(\"english-profanity.txt\") as file:\n",
        "    profanity_list=[]\n",
        "    for item in file:\n",
        "      profanity_list.append(item)\n",
        "# Create a dictionary which attempts to match the length of the profaned text with the actual text in the compiled list\n",
        "  profanity_dictionary={}\n",
        "  for i in range(len(profanity_list)):\n",
        "    word=profanity_list[i]\n",
        "    censored=\"*\"*int(len(word))\n",
        "    profanity_dictionary[word]=censored\n",
        "  for i in tqdm(range(0, len(selected_reviews))):\n",
        "    review=str(selected_reviews[i])\n",
        "    for profanity in profanity_dictionary:\n",
        "      review=review.replace(profanity_dictionary[profanity], profanity)\n",
        "    selected_reviews[i]=review\n",
        "  return (selected_reviews)"
      ],
      "metadata": {
        "id": "t33t50D6KACW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deal with URLs\n",
        "\n",
        "def remove_urls_https_and_www(selected_reviews):\n",
        "  new_reviews_list=[]\n",
        "  for i in tqdm(range(0, len(selected_reviews))):\n",
        "    url=re.compile(r\"url=https?://(www\\.)?\")\n",
        "    selected_reviews[i] = url.sub('', str(selected_reviews[i])).strip().strip('/')\n",
        "    url = re.compile(r\"https?://(www\\.)?\")\n",
        "    selected_reviews[i] = url.sub('', str(selected_reviews[i])).strip().strip('/')\n",
        "    url = re.compile(r\"www.?\")\n",
        "    selected_reviews[i] = url.sub('', str(selected_reviews[i])).strip().strip('/')\n",
        "    url = re.compile(r\".com\\S+\")\n",
        "    selected_reviews[i] = url.sub('', str(selected_reviews[i])).strip().strip('/')\n",
        "  return(selected_reviews)\n",
        "\n",
        "# deal with emojis\n",
        "def convert_emojis_to_word(selected_review):\n",
        "  for i in tqdm(range(len(selected_review))):\n",
        "    #print(type(selected_review[i]))\n",
        "    review=str(selected_review[i])\n",
        "    selected_review[i]=emoji.demojize(review, delimiters=(\"\", \" \"))\n",
        "  return selected_review\n",
        "\n",
        "# deal with emoticons \n",
        "# adopted from https://stackoverflow.com/questions/70935501/unbalanced-parenthesis-regex\n",
        "def convert_emoticons_to_word(selected_review):\n",
        "  for i in tqdm(range(0,len(selected_review))):\n",
        "    review=str(selected_review[i])\n",
        "    for emot in EMOTICONS_EMO:\n",
        "      review= review.replace(emot, EMOTICONS_EMO[emot].replace(\" \",\"_\"))\n",
        "    selected_review[i]=review\n",
        "  return selected_review\n",
        "\n",
        "# deal with casing\n",
        "def lower_case(selected_reviews):\n",
        "  new_reviews_list=[]\n",
        "  for i in tqdm(range(0, len(selected_reviews))):\n",
        "    selected_reviews_lower = selected_reviews[i].lower()\n",
        "    new_reviews_list.append(selected_reviews_lower)\n",
        "  return(new_reviews_list)\n",
        "\n",
        "# deal with punctuation\n",
        "def remove_punctuation(selected_reviews):\n",
        "  new_reviews_list=[]\n",
        "  for i in tqdm(range(0, len(selected_reviews))):\n",
        "    new_reviews = re.sub(r'[^A-Za-z0-9_]', ' ', str(selected_reviews[i]))\n",
        "    new_reviews_list.append(new_reviews)\n",
        "  return(new_reviews_list)\n",
        "\n",
        "\n",
        "# deal with stop-words\n",
        "def remove_stop_words(selected_reviews):\n",
        "  with open('stopwords-english') as f:\n",
        "    stopwords = f.read().splitlines()\n",
        "  for i in tqdm(range(0,len(selected_reviews))):\n",
        "    review=str(selected_reviews[i])\n",
        "    selected_reviews[i]=remove_stopwords(review)\n",
        "  return selected_reviews\n",
        "\n",
        "# deal with double spacing \n",
        "def remove_spacing(selected_reviews):\n",
        "  new_reviews_list=[]\n",
        "  for i in tqdm(range(0, len(selected_reviews))):\n",
        "    new_reviews = re.sub(r'\\s+', ' ', str(selected_reviews[i]), flags=re.I)\n",
        "    new_reviews_list.append(new_reviews)\n",
        "  return(new_reviews_list)\n"
      ],
      "metadata": {
        "id": "4vrw-G2_-iIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the above functions and save the file. \n",
        "\n",
        "FPS_reviews[\"review\"]=identify_profanities_1(list(FPS_reviews[\"review\"]))\n",
        "FPS_reviews[\"review\"]=identify_profanities_2(list(FPS_reviews[\"review\"]))\n",
        "FPS_reviews[\"review\"]=remove_urls_https_and_www(list(FPS_reviews[\"review\"]))\n",
        "FPS_reviews[\"review\"]=convert_emojis_to_word(list(FPS_reviews[\"review\"]))\n",
        "FPS_reviews[\"review\"]=convert_emoticons_to_word(list(FPS_reviews[\"review\"]))\n",
        "FPS_reviews[\"review\"]=lower_case(list(FPS_reviews[\"review\"]))\n",
        "FPS_reviews[\"review\"]=remove_punctuation(list(FPS_reviews[\"review\"]))\n",
        "FPS_reviews[\"review\"]=remove_stop_words(list(FPS_reviews[\"review\"]))\n",
        "FPS_reviews[\"review\"]=remove_spacing(list(FPS_reviews[\"review\"]))\n",
        "FPS_reviews=FPS_reviews.drop(['Unnamed: 0'], axis=1)\n",
        "\n",
        "FPS_reviews.to_csv('FPS_reviews_preprocessed.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1YDI-mvAIEx",
        "outputId": "9d30c786-087c-42dd-90ba-0fd3c39d8c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1056940/1056940 [00:11<00:00, 91013.50it/s]\n",
            "100%|██████████| 1056940/1056940 [00:16<00:00, 64093.48it/s]\n",
            "100%|██████████| 1056940/1056940 [00:12<00:00, 85040.03it/s]\n",
            "100%|██████████| 1056940/1056940 [01:52<00:00, 9375.90it/s]\n",
            "100%|██████████| 1056940/1056940 [01:39<00:00, 10637.67it/s]\n",
            "100%|██████████| 1056940/1056940 [00:01<00:00, 964271.36it/s]\n",
            "100%|██████████| 1056940/1056940 [00:18<00:00, 58325.39it/s]\n",
            "100%|██████████| 1056940/1056940 [00:14<00:00, 74868.27it/s]\n",
            "100%|██████████| 1056940/1056940 [00:11<00:00, 90317.27it/s]\n"
          ]
        }
      ]
    }
  ]
}
